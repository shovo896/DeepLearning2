{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f94b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 12:14:44.306644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cba91c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniPPO:\n",
    "    def __init__(self):\n",
    "        self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model=GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        self.tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "        self.optimizer=optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "        self.epsilon=0.2\n",
    "        self.memory = []  # Store good examples\n",
    "    \n",
    "    def calculate_reward(self, text, prompt):\n",
    "        \"\"\"Calculate reward based on relevance and text quality\"\"\"\n",
    "        reward = 0.0\n",
    "        words = text.strip().split()\n",
    "        text_lower = text.lower()\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Check if it's a question\n",
    "        is_question = '?' in prompt\n",
    "        \n",
    "        # Length reward (prefer reasonable length)\n",
    "        word_count = len(words)\n",
    "        if 5 <= word_count <= 15:\n",
    "            reward += 0.4\n",
    "        elif 1 <= word_count < 5:\n",
    "            reward += 0.2\n",
    "        else:\n",
    "            reward += 0.1\n",
    "        \n",
    "        # Relevance check based on prompt type\n",
    "        if is_question:\n",
    "            # For questions, reward direct answers\n",
    "            answer_starters = ['yes', 'no', 'i', 'of course', 'sure', 'definitely', \n",
    "                             'maybe', 'perhaps', 'i would', 'i will', 'i do']\n",
    "            if any(text_lower.strip().startswith(starter) for starter in answer_starters):\n",
    "                reward += 0.3\n",
    "            \n",
    "            # Check for question-specific keywords\n",
    "            if 'marry' in prompt_lower:\n",
    "                relevant_words = ['yes', 'no', 'love', 'forever', 'will', 'do', \n",
    "                                'course', 'definitely', 'honor', 'happy']\n",
    "                if any(word in text_lower for word in relevant_words):\n",
    "                    reward += 0.2\n",
    "        else:\n",
    "            # For statements, reward relevant continuation\n",
    "            prompt_words = set(prompt_lower.split())\n",
    "            text_words = set(text_lower.split())\n",
    "            overlap = len(prompt_words & text_words)\n",
    "            if overlap > 0:\n",
    "                reward += 0.2\n",
    "        \n",
    "        # Penalize irrelevant context switches (mentioning unrelated topics)\n",
    "        irrelevant_topics = ['car', 'town', 'mother', 'father', 'work', 'office']\n",
    "        if 'marry' in prompt_lower:\n",
    "            for topic in irrelevant_topics:\n",
    "                if topic in text_lower:\n",
    "                    reward -= 0.3\n",
    "                    break\n",
    "        \n",
    "        # Coherence - penalize excessive repetition\n",
    "        unique_words = len(set(words))\n",
    "        if word_count > 0 and unique_words / word_count < 0.6:\n",
    "            reward -= 0.2\n",
    "        \n",
    "        # Bonus for proper ending\n",
    "        if any(p in text for p in ['.', '!', '?']):\n",
    "            reward += 0.1\n",
    "        \n",
    "        # Ensure reward is between 0 and 1\n",
    "        return max(0.0, min(1.0, reward))\n",
    "    \n",
    "    def train_step(self, prompt):\n",
    "        self.model.train()\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1]+20,  # Shorter to keep responses focused\n",
    "            do_sample=True,\n",
    "            temperature=0.8,  # Lower temperature for more focused responses\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_tokens = outputs.sequences[0]\n",
    "        gen_tokens = generated_tokens[inputs.shape[1]:]\n",
    "        text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "        full_outputs = self.model(generated_tokens.unsqueeze(0), labels=generated_tokens.unsqueeze(0))\n",
    "        logits = full_outputs.logits[:, :-1, :]\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        current_log_probs = log_probs[0, inputs.shape[1]-1:generated_tokens.shape[0]-1].gather(1, gen_tokens.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Use improved reward calculation\n",
    "        reward = self.calculate_reward(text, prompt)\n",
    "        \n",
    "        reward_tensor = torch.tensor([reward], device=self.device)\n",
    "        loss = -(current_log_probs * reward_tensor).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Store examples with high reward\n",
    "        if reward > 0.5:\n",
    "            self.memory.append((prompt, text, reward))\n",
    "        \n",
    "        return text, reward\n",
    "    \n",
    "    def train_multiple_iterations(self, prompt, iterations=5):\n",
    "        \"\"\"Train on the same prompt multiple times to improve response\"\"\"\n",
    "        print(f\"Training on: '{prompt}'\\n\")\n",
    "        best_reward = 0\n",
    "        best_text = \"\"\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            text, reward = self.train_step(prompt)\n",
    "            print(f\"Iteration {i+1}/{iterations}\")\n",
    "            print(f\"  Response: {text}\")\n",
    "            print(f\"  Reward: {reward:.4f}\\n\")\n",
    "            \n",
    "            if reward > best_reward:\n",
    "                best_reward = reward\n",
    "                best_text = text\n",
    "        \n",
    "        print(f\"Best response: {best_text}\")\n",
    "        print(f\"Best reward: {best_reward:.4f}\")\n",
    "        return best_text, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "866fc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: 'Will you marry me?'\n",
      "\n",
      "Iteration 1/10\n",
      "  Response:  I-I-I'm-I-I-I'm-I'm-I'm-\n",
      "  Reward: 0.5000\n",
      "\n",
      "Iteration 2/10\n",
      "  Response:  I don't know.\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\"Don't you say so,\" she said\n",
      "  Reward: 1.0000\n",
      "\n",
      "Iteration 3/10\n",
      "  Response: \n",
      "\n",
      "Catherine: Yes.\n",
      "\n",
      "[Kylani starts to cry]\n",
      "\n",
      "K\n",
      "  Reward: 0.7000\n",
      "\n",
      "Iteration 4/10\n",
      "  Response: \n",
      "I don't know what I'm going to do with my life.\n",
      "So you're not\n",
      "  Reward: 1.0000\n",
      "\n",
      "Iteration 5/10\n",
      "  Response: \n",
      "\n",
      "This is what I want.\n",
      "\n",
      "I want to see this guy\n",
      "\n",
      "And I\n",
      "  Reward: 0.5000\n",
      "\n",
      "Iteration 6/10\n",
      "  Response: \n",
      "\n",
      "I'm your new wife, and you have a new home.\n",
      "\n",
      "\n",
      "No,\n",
      "  Reward: 1.0000\n",
      "\n",
      "Iteration 7/10\n",
      "  Response:  I think so.\"\n",
      "The woman looked up to me and said, \"I'll marry you.\n",
      "  Reward: 0.8000\n",
      "\n",
      "Iteration 8/10\n",
      "  Response:  No. I'm a human, you know. I'm not the only one who is trying to\n",
      "  Reward: 1.0000\n",
      "\n",
      "Iteration 9/10\n",
      "  Response:  No.\"\n",
      "\n",
      "\"Yeah. I can't wait to get married.\"\n",
      "She said.\n",
      "\n",
      "  Reward: 1.0000\n",
      "\n",
      "Iteration 10/10\n",
      "  Response: \n",
      "\n",
      "I'm the most talented person you know, but I'm not your dad.\n",
      "\n",
      "\n",
      "  Reward: 1.0000\n",
      "\n",
      "Best response:  I don't know.\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\"Don't you say so,\" she said\n",
      "Best reward: 1.0000\n"
     ]
    }
   ],
   "source": [
    "ppo = MiniPPO()\n",
    "prompt = \"Will you marry me?\"\n",
    "best_text, best_reward = ppo.train_multiple_iterations(prompt, iterations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
